---
title: "强化学习第三章：基于模型的动态规划方法"
author: "马俏颖"
date: "2022-11-27"
output: html_document
---


## 3 基于模型的动态规划方法

#### 3.1 基于模型的动态规划方法理论
&emsp;
上⼀章我们将强化学习的问题纳⼊到⻢尔科夫决策过程的框架下解决。       

&emsp;
⼀个完整的已知模型的⻢尔科夫决策过程可以利⽤元组$(S，A，P，r，γ)$来表⽰。其中S为状态集，A为动作集，P为转移概率，也就是对应着环境和智能体的模型，r为回报函数，γ为折扣因⼦⽤来计算累积回报R。

&emsp;
累积回报公式为$R=\sum_{t=0}^{T}\gamma ^{t}r_{t},0\leqslant \gamma \leqslant 1$.

&emsp;
当T为有限值时，强化学习过程称为有限范围强化学习，当T=∞时，称为⽆穷范围强化学习。我们以有限
范围强化学习为例进⾏讲解。
&emsp;
强化学习的⽬标是找到最优策略π使得累积回报的期望最⼤。所谓策略
是指状态到动作的映射π：s→a，⽤τ表⽰从状态s到最终状态的⼀个序列
τ：st ，st+1 …，sT ，则累积回报R （τ）是个随机变量，随机变量⽆法进
⾏优化，⽆法作为⽬标函数，我们采⽤随机变量的期望作为⽬标函数，即
∫R （τ）pπ （τ） dτ作为⽬标函数。⽤公式来表⽰强化学习的⽬标：
。强化学习的最终⽬标是找到最优策略为π
* ：s→u
*
，我们看⼀下这个表达式的直观含义。
如图3.1所⽰，最优策略的⽬标是找到决策序列 ，因
此从⼴义上来讲，强化学习可以归结为序贯决策问题。即找到⼀个决策序
列，使得⽬标函数最优。这⾥的⽬标函数∫R （τ）pπ （τ） dτ是累积回报的
期望值，累积回报的含义是评价策略完成任务的总回报，所以⽬标函数等
价于任务。强化学习的直观⽬标是找到最优策略，⽬的是更好地完成任
务。回报函数对应着具体的任务，所以强化学习所学到的最优策略是与具
体的任务相对应的。从这个意义上来说，强化学习并不是万能的，它⽆法
利⽤⼀个算法实现所有的任务。
