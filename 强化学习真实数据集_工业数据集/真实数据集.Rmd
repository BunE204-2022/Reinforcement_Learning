---
title: "强化学习数据集"
author: "欧阳露露"
date: "2023-05-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### 数据集
![](D:/强化学习/两个数据集简图.png)

- 数据集a：slate recommendation （石板推荐？）

- 数据集b：sequential slate recommendation （顺序石板推荐？）


&emsp;&emsp;在每一秒内，推荐引擎应该用3个商品列表响应用户的请求（每个列表3个商品），下一个商品列表被锁定，直到当前列表的商品售罄，我们称之为“解锁”规则。

&emsp;&emsp;如果用户对现有的项目列表不满意，他们可以通过刷新按钮刷新页面（即项目列表），每天最多刷新3次。

- 数据集a表示为RL4RS-Slate，数据集b表示为RL4RS-SeqSlate 

&emsp;&emsp;RL4RS-Slate侧重于石板推荐。它将用户在单个页面上的行为视为一个MDP过程。

&emsp;&emsp;RL4RS-SeqSlate侧重于顺序石板推荐。它不仅考虑如何推荐单个页面，而且考虑页面之间的关系，以最大化会话的总奖励。

&emsp;&emsp;对于每个数据集，还可以获得强化学习（RL）部署前后的分离数据。
如Slate-SL和Slate-RL。在此基础上，以Slate-SL作为训练集，Slate-RL作为测试集，用于评估不同批次RL算法的有效性，并测量了外推误差的程度     

&emsp;&emsp;我们的得到的数据集：rl4rs_dataset_a_rl，rl4rs_dataset_a_sl，rl4rs_dataset_b_rl，rl4rs_dataset_b_sl


### 推荐系统框架
![](D:/强化学习/收集数据的推荐系统架构.png)

- 推荐引擎包含五个主要组件:RS代理、项目池、日志记录中心、训练成分和在线KV系统。

- 系统的工作流程主要包括两个循环:在线规划循环和在线学习循环。

- 左下角所示，在线规划循环是推荐代理与用户交互的循环。我们将记录每次会话的项目信息和用户对每个项目的行为。

- 学习循环(在图的右侧)，训练过程发生；每当更新模型时，将其改写为在线KV系统。届时我们将记录网络架构和网络参数。两个工作回路通过日志中心和在线KV系统连接。我们将为每个用户日志记录在线KV系统中存储的相应用户和项目特征。


### 原始日志数据

原始日志数据的格式为:

(1)时间:事件发生的时间。


(2)会话ID:唯一标识用户会话的唯一编号。


(3)序列ID:一个唯一的数字，表示状态在会话中的位置(即页面顺序)。


(4)商品:一个长度为9个空格分隔的列表，表示向用户推荐的9个商品(从左到右，从上到下)。


(5)用户反馈:一个长度为9个空格分隔的列表，表示用户对九个推荐商品的反应(从左到右，从上到下)。用户反馈(0/1)乘以每个商品的效用将被用作该页面的奖励。


(6)用户特征:匿名用户特征包括42元用户画像和64元用户点击历史记录。


(7)商品特征:360元项目特征描述了本页上推荐商品的上下文，例如商品的id、商品的类别、商品嵌入和历史点击率。


(8)行为策略ID:记录该行为策略(基于监督学习的策略)当时的网络架构和网络参数的模型文件。


### 强化学习应用到数据集
![](D:/强化学习/基于强化学习推荐系统的MDP框架.png)


one episode一幕数据，MDP

状态States：每一步的状态$s_t\in S$
不仅需要反映静态用户偏好，还需要反映动态上下文信息。对于前者，我们用用户画像的静态用户特征和用户对物品的历史偏好来表示。后者由上下文特征表示，包括所选商品的特征$a_i$和动态统计直到步骤

动作Actions：在每个状态$s_t$，agent的一个动作是从所有有效的商品中选择一个商品$a_t$进行推荐。一个商品可以用它的ID或它的商品嵌入来表示。我们测试了离散和连续动作设置。

转移Transition：环境(即用户)在所有操作完成后给出反馈，其中只有操作$a_t$被添加到用户上下文中，其他操作(例如用户偏好)等于它们在$x_t$中的当前值。因此，通过将$s_t$和项目$a_t$，可以确定地获得下一个状态$s_{t+1}$。

回报reward：用户对某件物品的反应的奖励功能，页面定义为
$r_{page} = r(s_{page})=r(s_0,a_0,a_1,\cdots,a_{page-1})=\sum_{i=0}^{page-1}\gamma^ip_ir_i$，即用户在该页面上的期望效用，$\gamma$其中为折扣因子，$p_i$表示在整个页面条件下第i项的购买概率，$r_i$表示第i项的效用。给定步长限制$T$，强化学习的目标是最大化所有幕的预期累积效用，即所有商品页面的效用总和。

策略Policy：随机策略表示给定状态下所有可能物品的条件概率分布。从$s_0$开始，它根据每个步骤$t$的状态$s_t$迭代地推荐一个物品$a_t$，直到达到步骤限制$T$。

总结，MDP中用到原始日志的数据：   
状态中:（6）用户特征，（7）项目特征；       
动作:（7）项目特征     
奖励/回报:（6）用户特征，（7）项目特征



### 查看数据集

#### 数据集a的训练集
```{r}
data_a_sl <- read.csv("D:/强化学习/rl4rs-dataset/rl4rs_dataset_a_rl.csv", header=T,
   sep="@", row.names="session_id")
head(data_a_sl,3)
```


#### 数据集a的测试集
```{r}
data_a_rl <- read.csv("D:/强化学习/rl4rs-dataset/rl4rs_dataset_a_rl.csv", header=T,
   sep="@", row.names="session_id")
head(data_a_rl,3)
```

