---
title: "2 马尔科夫决策过程"
author: "吴羽暄"
date: "2023/6/18"
output: html_document
---

## 2 马尔科夫决策过程MDP

#### 2.1 MDP理论讲解

强化学习的学习过程是动态的、不断交互的过程，所需要的数据也是通过与环境不断交互所产生的。所以，与监督学习和⾮监督学习相⽐，强化学习涉及的对象更多，比如动作，环境，状态转移概率和回报函数等。

组成元素：(S，A，P，R，γ)

S 为有限的状态集

A 为有限的动作集

P 为状态转移概率

R 为回报函数

γ 为折扣因⼦，⽤来计算累积回报

1.马尔科夫性：$P[s_{t+1}|s_t]=P[s_{t+1}|s_1,\dots,s_t]$

(1) 描述每个状态(S)的性质:系统的下⼀个状态$s_{t+1}$仅与当前状态$s_t$有关，而与以前的状态无关。实际上，当前状态$s_t$其实蕴含了所有相关的历史信息$s_1,\dots,s_t$,一旦当前状态已知，历史信息将会被抛弃。

(2) 描述一个状态序列，如：$s_1→s_2→s_3→s_4→s_5$。数学中描述随机变量序列即随机过程。若随机变量序列中的每个状态都是马尔科夫的，则称此随机过程为马尔科夫随机过程。

2.马尔科夫过程：(S,P)

S是有限状态集合，P是状态转移概率。状态转移概率矩阵为：$$P=
\left [
\begin{matrix}
P_{11} & \cdots & P_{1n}  \\
\vdots & \vdots & \vdots   \\
P_{n1} & \cdots & P_{nn}  
\end{matrix} \right ]
,$$

图2.2

如图2.2所示为一个学生的7种状态{娱乐，课程1，课程2，课程3，考过，睡觉，论⽂}，每种状态之间的转换概率如图所示。则该生从课程1开始一天可能的状态序列为：

课1-课2-课3-考过-睡觉

课1-课2-睡觉

以上状态序列称为马尔科夫链。当给定状态转移概率时，从某个状态出发存在多条马尔科夫链。对于游戏或者机器⼈，马尔科夫过程不足以描述其特点，因为不管是游戏还是机器⼈，他们都是通过动作与环境进行交互，并从环境中获得奖励，而马尔科夫过程中不存在动作和奖励。将动作（策略）和回报考虑在内的马尔科夫过程称为马尔科夫决策过程。

3.马尔科夫决策过程：$P_{ss'}^a=P[S_{t+1}=s'|S_t=s,A_t=a]$

(1) 描述MDP：{S,A,P,R,$\gamma$}

图2.3

马尔科夫过程只描述状态S，而MDP的状态转移概率还包含动作A。如图2.3所示，学生的：

状态集为S={$s_1,s_2,s_3,s_4,s_5$}

动作集为A={玩，退出，学习，发表，睡觉}

用R表示图2.3中(每个动作对应的)立即回报。

累积回报$G_t=R_{t+1}+\gamma R_{t+2}+\cdots=\sum_{k=0}^ \infty \gamma^k R_{t+k+1}$(注：给定策略π时可以计算)

(2) 随机策略

强化学习的目标是给定MDP，寻找最优即总回报最大的策略。

策略：一个条件概率。即$\pi(a|s)=p[A_t=a|S_t=s]$，状态到动作的映射，即策略π在每个状态s指定⼀个动作a概率或给定状态s时，动作集上的⼀个分布。常用符号π表⽰。

例如其中一个学生的策略为$\pi_1(玩|s_1)$=0.8，即该学生在状态$s_1$时玩的概率为0.8，不玩的概率是0.2，显然这个学生更喜欢玩。另外一个学生的策略为$\pi_2(玩|s_1)$=0.3，即该学生在状态$s_1$时玩的概率是0.3，显然这个学生不爱玩。依此类推，每个学生都有自己的策略。

当给定策略π时，假设从状态$s_1$出发，学生状态序列可能为

$s_1→s_2→s_3→s_4→s_5$

$s_1→s_2→s_3→s_5$

$\vdots$

此时，在策略π下，利⽤累计回报公式可以计算累积回报$G_1$ ，此时$G_1$有多个可能值。由于策略π是随机的，因此累积回报也是随机的。

为了评价状态$s_1$的价值，我们需要定义一个确定量来描述状态$s_1$的价值，很自然的想法是利用累积回报来衡量状态$s_1$的价值。然⽽，累积回报$G_1$是个随机变量，不是一个确定值，因此无法描述，但其期望是个确定值，可以作为状态值函数的定义。

(3) 状态值函数与状态-行为值函数的定义式

状态值函数：累积回报在状态s处的期望值
$$\begin{aligned}
v_\pi(s)&=E_\pi[G_t|S_t=s]\\
&=E_\pi[\sum_{k=0}^\infty\gamma^kR_{t+k+1}|S_t=s]\\
\end{aligned}$$
因为策略π决定了累计回报G的状态分布，故状态值函数是与策略π相对应的。

状态-行为值函数：累积回报在状态s及动作a处的期望值
$$\begin{aligned}
q_\pi(s,a)&=E_\pi[G_t|S_t=s,A_t=a]\\
&=E_\pi[\sum_{k=0}^\infty\gamma^kR_{t+k+1}|S_t=s,A_t=a]\\
\end{aligned}$$

(4) 状态值函数与状态-行为值函数的贝尔曼方程

$$\begin{aligned}
v_\pi(S_t)&=E[G_t|S_t=s]\\
&=E[R_{t+1}+\gamma v(S_{t+1})|S_t=s]\\
&=\sum_{a\in A}\sum_{s'\in S} \pi(a|s)(R_s^a+\gamma P_{ss'}^av_\pi(s'))\\
\end{aligned}$$

$$\begin{aligned}
q(S_t,A_t)&=E[G_t|S_t=s,A_t=a]\\
&=E[R_{t+1}+\gamma q(S_{t+1},A_{t+1})|S_t=s,A_t=a]\\
&=R_s^a+\gamma \sum_{s'\in S}\sum_{a'\in A}P_{ss'}^a \pi(a'|s')q_\pi(s',a')\\
\end{aligned}$$
表明了当前状态s值函数与下个状态s'值函数的关系。实际上给出了计算值函数的方法：迭代/递归

因为$G_t=R_{t+1}+\gamma G_{t+1}$，故t时刻计算的值函数必然和t+1时刻的值函数存在关系

图2.4

(5)最优状态值函数、状态-行为值函数及最优策略
$$\begin{aligned}
v^*(s)&=\max_\pi v_\pi(s)\\
&=\max_a R_s^a+\gamma \sum_{s' \in S}P_{ss'}^av^*(s')\\
\end{aligned}$$

$$\begin{aligned}
q^*(s,a)&=\max_\pi q_\pi(s,a)\\
&=R_s^a +\gamma \sum_{s' \in S}P_{ss'} \max_{a'}q^*(s',a')\\
\end{aligned}$$
最优策略选择：$$\pi^*(a|s)=\begin{cases}1\quad if \quad a=\arg \max \limits_ {a\in A} q^*(s,a) \\0\quad othewise\end{cases}$$


#### 2.2 MDP中的概率学基础讲解

1. 理解随机策略$\pi(a|s)=p[A_t=a|S_t=s]$

(1)随机变量：MDP中随机变量指的是当前的动作，用字母a表⽰，可以是离散也可以是连续的。

(2)概率分布：⽤来描述随机变量在每个可能取到的值处的可能性大小。指定一个策略π就是指定取每个动作a的概率。

(3)条件概率：策略$\pi(a|s)$指在当前状态π处，采取某个动作a的概率。当给定随机变量后，状态s处的累积回报G(s)也是随机变量，而且其分布由随机策略π决定。状态值函数定义为该累积回报的期望。

(4)期望

离散型：$$E_{x\sim P}[f(x)]=\sum_xP(x)f(x)$$
连续型：$$E_{x\sim P}[f(x)]=\int p(x)f(x)dx$$

(5)方差：采样值差异的大小$$Var(f(x))=E[(f(x)-E[f(x)])^2]$$

2. 常用的随机策略(概率分布)

(1) 贪婪策略
$$\pi^*(a|s)=\begin{cases}1 \quad if \quad a=\arg \max \limits_ {a\in A} q^*(s,a) \\0 \quad othewise\end{cases}$$一个确定性策略，即只有在使得状态-动作值函数最大的动作a处取概率为1。

(2) $\epsilon-greedy$策略
$$\pi(a|s)\leftarrow\begin{cases}1-\epsilon+\frac{\epsilon}{|A(s)|} \quad if \quad a=\arg \max \limits_ {a\in A} Q(s,a) \\0 \quad\quad\quad\quad\quad\quad if \quad a\neq \arg \max \limits_ {a\in A} Q(s,a)\end{cases}$$选取使得状态-动
作值函数最大的动作的概率为$1-\epsilon+\frac{\epsilon}{|A(s)|}$，而其他动作概率为等概率$\frac{\epsilon}{|A(s)|}$

(3) 高斯策略
$$\pi_\theta=\mu_\theta+\epsilon,\epsilon\sim N(0,\sigma^2)$$其中，$\mu_0$为确定性部分，$\epsilon$为零均值的高斯随机噪声，应用于连续型变量。

(4) 玻尔兹曼分布
$$\pi(a|s,\theta)= \frac{exp(Q(s,a,\theta))}{\sum_bexp(h(s,b,\theta))} $$其中Q(s,a,$\theta$)为动作值函数。应用于动作空间是离散的或者动作空间并不大的情况。该策略的含义是，动作值函数大的动作被选中的概率大，动作值函数小的动作被选中的概率小。

#### 2.3 基于gym的MDP实例讲解

见代码grid_mdp

