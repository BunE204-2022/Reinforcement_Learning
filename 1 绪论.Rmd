---
title: "1 绪论"
author: "吴羽暄"
date: "2023/6/18"
output: html_document
---

## 1 绪论

#### 1.1 这是一本什么书

2016年和2017年最具影响力的AlphaGo大胜世界围棋冠军李世石和柯洁事件，其核心算法就用到了强化学习算法。

本书特点：

第一，本书的语言风格偏口语化；

第二，每章节会接受对应的数学知识，便于理解；

第三，每部分都包括理论讲解，代码讲解和直观解释三项内容（我：先讲理论，再讲代码）；

第四，本书涵盖的内容相当丰富，几乎会涉及强化学习算法的各个方面。

#### 1.2 强化学习可以解决什么问题

序贯决策问题：需要连续不断地做出决策，才能实现最终目标的问题。

图1.1

A:该系统由一个台车（黑体矩形）和两个摆（红色摆杆）组成，可控制的输入为台车的左右运动，该系统的目的是让两级摆稳定在竖直位置。两级摆问题是非线性系统的经典问题，在控制系统理论中，解决该问题的基本思路是先对两级摆系统建立精确的动力学模型，然后基于模型和各种非线性的理论设计控制方法。它需要在每个状态下都有个智能决策（在这里智能决策是指应该施加给台车什么方向、多大的力），以便使整个系统逐渐收敛到目标点（也就是两个摆竖直的状态）。

B:训练好的AlphaGo与柯洁对战的第二局棋。AlphaGo需要根据当前的棋局状态做出该下哪个子的决策，以便赢得比赛。

C:机器⼈在仿真环境下自己学会了从摔倒的状态爬起来。机器人需要得到当前状态下每个关节的力矩，以便能够站立起来。

其他领域：视频游戏、人机对话、无人驾驶、机器翻译、文本序列预测等。

综上：强化学习可以解决序贯决策问题。

#### 1.3 强化学习如何解决问题

1.区别于“监督学习”

智能感知问题：以数字手写体识别为例。智能感知其实就是在学习“输入”长得像什么（特征），以及与该长相一一对应的是什么（标签）。所以，智能感知必不可少的前提是需要大量长相差异化的输入以及与输入相关的标签。因此，监督学习解决问题的方法就是输入大量带有标签的数据，让智能体从中学到输入的抽象特征并分类。

序贯决策问题：不关心输入长什么样，只关心当前输入下应该采用什么动作才能实现最终的目标。再次强调，当前采用什么动作与最终的目标有关。也就是说当前采用什么动作，可以使得整个任务序列达到最优。要使整个任务序列达到最优，需要智能体不断地与环境交互，不断尝试。智能体通过动作与环境进行交互时，环境会返给智能体一个当前的回报，智能体则根据当前的回报评估所采取的动作：有利于实现目标的动作被保留，不利于实现目标的动作被衰减。

图1.3

用一句话来概括强化学习和监督学习的异同点：强化学习和监督学习的共同点是两者都需要大量的数据进行训练，但是两者所需要的数据类型不同。监督学习需要的是多样化的标签数据，强化学习需要的是带有回报的交互数据。由于输⼊的数据类型不同，这就使得强化学习算法有它自己的获取数据、利用数据的独特方法，这些方法在后面的章节中会一一介绍。

2.强化学习算法的发展历史

(1) 第一个关键点是1998年，标志性的事件是Richard S.Sutton出版了他的强化学习导论第一版，即Reinforcement Learning：An Introduction。该书系统地总结了1998年以前强化学习算法的各种进展。在这一时期强化学习的基本理论框架已经形成。1998年之前，学者们关注和发展得最多的算法是表格型强化学习算法。这一时期基于直接策略搜索的方法也被提出来了。如1992年R.J.Williams提出了Rinforce算法直接对策略梯度进行估计。

(2) 第二个关键点是2013年DeepMind提出DQN(Deep Q Network)，将深度网络与强化学习算法结合形成深度强化学习。从1998年到2013年，学者们也没闲着，发展出了各种直接策略搜索的方法。2013年之后，随着深度学习的火热，深度强化学习也越来越引起大家的注意。尤其是2016年和2017年，谷歌的AlphaGo连续两年击败世界围棋冠军，更是将深度强化学习推到了风口浪尖之上。

#### 1.4 强化学习算法分类及发展趋势

1.算法分类

(1) 根据算法是否依赖模型，分为基于模型的强化学习算法和无模型的强化学习算法。共同点：通过与环境交互获得数据；不同点：利用数据的方式不同。基于模型的强化学习算法利用与环境交互得到的数据学习系统或者环境模型，再基于模型进行序贯决策；无模型的强化学习算法则是直接利用与环境交互获得的数据改善自身的行为。两类方法各有优缺点，一般来讲基于模型的强化学习算法效率要比无模型的强化学习算法效率更⾼，因为智能体在探索环境时可以利用模型信息。但是，有些根本无法建立模型的任务只能利用无模型的强化学习算法。由于无模型的强化学习算法不需要建模，所以和基于模型的强化学习算法相比，更具有通用性。

(2) 根据策略的更新和学习方法，强化学习算法可分为基于值函数的强化学习算法、基于直接策略搜索的强化学习算法以及AC的方法。基于值函数的强化学习方法是指学习值函数，最终的策略根据值函数贪婪得到，即任意状态下，值函数最大的动作为当前最优策略。基于直接策略搜索的强化学习算法，一般是将策略参数化，学习实现目标的最优参数。基于AC的⽅法则是联合使用值函数和直接策略搜索。具体的算法会在后面介绍。

(3) 根据环境返回的回报函数是否已知，可以分为正向强化学习和逆向强化学习。在强化学习中，回报函数是人为指定的，回报函数指定的强化学习算法称为正向强化学习。很多时候，回报无法人为指定，如无人机的特效表演，这时可以通过机器学习的方法由函数自己学出来回报。

(4) 本书之外：分层强化学习、元强化学习、多智能体强化学习、关系强化学习和迁移强化学习等。

2.发展趋势

第一，强化学习算法与深度学习的结合会更加紧密。机器学习算法常被分为监督学习、非监督学习和强化学习，如今三类方法联合起来使用效果会更好，谁结合得好，谁就会有更好的突破。这一方向的代表作如基于深度强化学习的对话生成等；

第二，强化学习算法与专业知识结合得将更加紧密；

第三，强化学习算法理论分析会更强，算法会更稳定和高效；

第四，强化学习算法与脑科学、认知神经科学、记忆的联系会更紧密。脑科学和认知神经科学一直是机器学习灵感的源泉，这个源泉往往会给机器学习算法带来革面性的成功。

#### 1.5 强化学习仿真环境构建

学习算法的共同点是从数据中学习，因此数据是学习算法最基本的组成元素。监督学习的数据独立于算法本身，而强化学习的数据是智能体与环境的交互数据，在交互中智能体逐渐地改善行为，产生更好的数据，从而学会技能。也就是说强化学习的数据跟算法是交互的，而非的。因此，相比于监督学习只构建一个学习算法，强化学习还需要构建一个用于与智能体进行交互的环境。

仿真环境必备的两个要素：物理引擎和图像引擎。物理引擎用来计算仿真环境中物体是如何运动的，其背后的原理是物理定律，如刚体动力学，流力学和柔性体动力学等。常用的开源物理引擎有ODE(Open Dynamics Engine)、Bullet、Physx和Havok等。图像引擎则用来显示仿真环境中的物体，包括绘图、渲染等。常用的图像引擎大都基于OpenGL(Open Graphics Library)。

本书所用仿真环境为OpenAI的gym。安装使用gym的基本流程：

安装Anaconda(注意，要将路径安装到环境变量中)

→用Anaconda创建虚拟环境(本书只给了Linux系统的安装方法，Windows和Mac自行上网搜)

→安装gym(先激活，再安装各种包)

→下载python/pycharm，将环境设置为gym，import gym即可

具体见代码code及cartpole

#### 1.6 本书主要内容及安排

写作线索：第一条线索是强化学习的基本算法，第二条线索是强化学习算法所用到的基础知识。

1.第一条线索：强化学习的基本算法

强化学习算法解决的是序贯决策问题，而一般的序贯决策问题可以利用马尔科夫决策过程的框架来表述，因此在第2章中介绍了MDP。MDP能够用数学的形式将要解决的问题描述清楚，这也是为什么在介绍强化学习时首先要讲MDP的原因。

第3章介绍基于动态规划的强化学习算法，即对于模型已知的MDP问题的解，并由此引出广义策略迭代的⽅法。广义策
略迭代方法不仅适用于基于模型的⽅法，也适用于无模型的⽅法，是基于值函数强化学习算法的基本框架。因此，第3章是第4章基于蒙特卡罗方法、第5章基于时间差分方法和第6章基于值函数逼近方法的基础。

第4章介绍基于蒙特卡罗的强化学习算法。无模型的强化学习算法是整个强化学习算法的核心，而基于值函数的强化学习算法的核心是计算值函数的期望。值函数是个随机变量，其期望的计算可通过蒙特卡罗的方法得到。

第5章介绍时间差分方法。基于蒙特卡罗的强化学习算法通过蒙特卡罗模拟计算期望，该方法需要等到每次试验结束后再对值函数进行估计，收敛速度慢。时间差分的方法则只需要一步便更新，效率高、收敛速度快。

第6章介绍基于值函数逼近的强化学习算法。第4章到第5章介绍的是表格型强化学习。所谓表格型强化学习是指状态空间和动作空间都是有限集，动作值函数可用一个表格来描述，表格的索引分别为状态量和动作量。但是，当状态空间和动作空间很大，甚至两个空间都是连续空间时，动作值函数已经无法使用一个表格来描述，这时可以用函数逼近理论对值函数进行逼近。

第7章开始介绍强化学习算法的第二大类：直接策略搜索方法。第7章介绍策略梯度理论、第8章介绍TRPO方法、第9章介绍确定性策略搜索。

第7章到第9章，介绍的是无模型的直接策略搜索方法。对于机器⼈等复杂系统，无模型的方法随机初始化很难找到成功的解，因此算法难以收敛。这时，可以利用传统控制器来引导策略进⾏搜索。因此第10章介绍了基于引导策略搜索的强化学习算法、为了学习回报函数，第11章介绍了逆向强化学习的算法。

从第12章开始，我们介绍了最近发展出来的强化学习算法，分别是第12章的组合策略梯度和值函数⽅法，第13章的值迭代网络和第14章的PILCO方法及其扩展。

2.第二条线索：强化学习算法所用到的基础知识

概率学基础、线性方程组的数值求解方法：高斯-赛德尔迭代法、时变与泛函分析中的压缩映射、统计学中的重要技术，如重要性采样、拒绝性采样和MCMC方法、基本的函数逼近方法：基于非参数的函数逼近和基于参数的函数逼近：如卷积神经网络、基本的信息论概念和基本的优化方法、大型监督算法常⽤的LBFGS优化算法，及其学习中的并行优化算法ADMM算法和KL散度及变分推理等。

